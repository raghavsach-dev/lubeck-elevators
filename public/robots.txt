# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
#
# To control which paths to disallow, remove the wildcard ('*') and specify a user-agent.
# For example, to disallow crawling of the entire site by Google's crawler:
#
# User-agent: Googlebot
# Disallow: /
#
# To allow all crawlers to access all paths, use the following:
User-agent: *
Disallow: /api/ # Disallow crawling of API routes

# Allow crawling of all other directories
Allow: /

# Point to the sitemap file
Sitemap: https://lubeckelevators.com/sitemap.xml

# Priority pages for crawling
Allow: /about
Allow: /contact
Allow: /products/
Allow: /gallery
Allow: /videos
Allow: /clients
Allow: /certifications

# Crawl delay (optional, for being respectful to server resources)
Crawl-delay: 1

# Block specific user agents if needed (currently allowing all)
# User-agent: BadBot
# Disallow: /

# Block admin or private areas (if any exist in the future)
# Disallow: /admin/
# Disallow: /private/

# Allow search engines to crawl all images
User-agent: Googlebot-Image
Allow: /Products/
Allow: /Gallery/
Allow: /Clientage/
Allow: /Certifications/

# Allow all static assets
Allow: /*.css$
Allow: /*.js$
Allow: /*.png$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.gif$
Allow: /*.svg$
Allow: /*.webp$
Allow: /*.avif$
Allow: /*.pdf$
Allow: /*.mp4$ 